{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS613: Natural language and Processing\n",
    "# Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Word Embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is collective name of techniques used in natual language processing to convert words or phrases from a vocabulary into vectors of real numbers. We convert words to vectors so that the machine can understand it. We want that words having similar meaning should have similar representations. Word embeddings help us to map words to a high dimensional space such that the words that are similar are closer to each other.\n",
    "\n",
    "<img src=\"vector-model1.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to measure similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sim.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 600 />\n",
    "\n",
    "### 1. Manhattan Distance\n",
    "<img src=\"manhattan.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 200 />\n",
    "Manhattan distance for two vectors $x$ and $y$ is defined as follows:\n",
    "<img src=\"manhattan_formula.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 150 />\n",
    "It is based on the distance between a grid like structure and is the sum of horizontal and vertical distances between two points on a grid. It is also known as L1 norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhatttan Distance: 4\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "def manhattan_distance(x,y):\n",
    "    l = len(x)\n",
    "    distance = 0\n",
    "    for i in range(l):\n",
    "        distance += abs(x[i] - y[i])\n",
    "    return distance\n",
    "\n",
    "print(\"Manhatttan Distance:\", manhattan_distance([1,0,0], [0,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Euclidean Distance\n",
    "<img src=\"euclidean_distance.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 300 />\n",
    "Euclidean distance for two vectors $x$ and $y$ is defined as follows:\n",
    "<img src=\"euclid_eqn.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 200 />\n",
    "It is the straight line distance between two points. It is also known as L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 2.449489742783178\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "import math\n",
    "def euclidean_distance(x,y):\n",
    "    l = len(x)\n",
    "    distance = 0\n",
    "    for i in range(l):\n",
    "        distance += (x[i] - y[i])**2\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "print(\"Euclidean Distance:\", euclidean_distance([1,0,0], [0,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cosine Similarity\n",
    "<img src=\"cosine.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 350 />\n",
    "Cosine similarity between two vectors $x$ and $y$ is defined as follows:\n",
    "<img src=\"eqn_similarity.svg\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 350 />\n",
    "It is the based on the angle between two vectors and does not take into account the magnitude of the two vectors. Smaller is the angle, greater is the similarity. \n",
    "We want that similar words occupy close spatial positions i.e. they have small angle between them. Mathematically, we want that the cosine of angle between such vectors should be close to 1. This also allows us to find relations such a word is opposite of another word if cosine similarity is close to -1.  Cosine similarity is thus widely used as a measure of similarity between two words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "import numpy as np\n",
    "def cosine_similarity(x, y):\n",
    "    norm_x = math.sqrt(np.sum(np.dot(x,x))) #||x||\n",
    "    norm_y = math.sqrt(np.sum(np.dot(y,y))) #||y||\n",
    "    if(norm_x == 0 or norm_y == 0):\n",
    "        print(\"Error: One of the vectors is zero\")\n",
    "        return -100000000\n",
    "    distance = np.dot(x,y) / (norm_x * norm_y) #calculating similarity\n",
    "    return distance\n",
    "print(\"Cosine Similarity:\", cosine_similarity([1,0,0], [1,2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to convert words into vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various techniques to convert a word into vector such as  neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, word2vec, glove vectors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive method - One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is the traditional way of representing a word as a vector. \n",
    "<img src=\"onehot.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 350 />\n",
    "It has 1 at only one position and zeroes at all other positions. The length of the vector is equal to the size of the vocabulary. Vocabulary is the number of unique words in our corpus. We order these words. We put one at the position for a word according to the ordering. Usually the words are ordered in aphabetical order. <br>\n",
    "For example: Vocabualry = \\[apple, banana, orange\\]. <br> Then one hot encoding are v_apple = [1,0,0], v_banana = [0,1,0], v_orange = [0,0,1]  <br>\n",
    "<br>\n",
    "Issues with this approach:\n",
    "1. We can not infer any relationship between two words. For example words happy and joy are similar but the cosine similarity will be zero. Also the euclidean distance will be the same for any two words. \n",
    "2. The vectors are very sparse i.e. they consist of a lot of zeroes. Thus, a lot of space is wasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec takes into account the context(i.e. the surrounding words) of the word to find the vector of a word. There are two types of word2vec algorithms: Skip-gram, Continuous Bag of Words(CBOW). \n",
    "#### a. Skip-gram\n",
    "In this we try to predict the context words based on the target word.\n",
    "<img src=\"skipgram.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 400 />\n",
    "For example if the sentence is _I am in a happy mood_ and the target word is _happy_ then the content words are _I, am, in, a, mood_ assuming the window size is 9. Window size is the number of surrounding words we take as our context. The input to the model is one hot vector of _happy_ and we try to learn the one hot encodings of the context words. In the learning process we learn the vector representation of the target word from the input layer and the weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture of skip-gram model:<br>\n",
    "a. $W_{N,M}$ is the wieght matrix that maps the input to hidden layer. <br>\n",
    "b. $W'_{M,K}$ is the wieght matrix that maps the hidden layer to the output layer. <br>\n",
    "\n",
    "At the output layer we apply softmax and thresholding to get one hot vectors. <br>\n",
    "The details of the model can be found here: __[Word2Vec](https://arxiv.org/pdf/1411.2738.pdf)__<br>\n",
    "<br>\n",
    "In skip gram the length of the word vectors reduces from the size of vocabulary to the dimension of the the hidden layer i.e. M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implementation of skip-gram__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has various corpus from the gutenberg project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "for i in (nltk.corpus.gutenberg.fileids()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take the shakespeare-ceaser.txt corpus. We preprocess the corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample sentence:\n",
      " ['be', 'gone', ',', 'runne', 'to', 'your', 'houses', ',', 'fall', 'vpon', 'your', 'knees', ',', 'pray', 'to', 'the', 'gods', 'to', 'intermit', 'the', 'plague', 'that', 'needs', 'must', 'light', 'on', 'this', 'ingratitude']\n"
     ]
    }
   ],
   "source": [
    "#performing case loweringZz\n",
    "corpora = nltk.corpus.gutenberg.sents('shakespeare-caesar.txt') #taking sentences of the corpus\n",
    "corpus = []\n",
    "for sentence in corpora:\n",
    "    sent = []\n",
    "    for word in sentence:\n",
    "        sent.append(word.lower())\n",
    "    corpus.append(sent)\n",
    "print(\"A sample sentence:\\n\", corpus[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'because', 'same', \"you'll\", 'o', 'yourselves', 'through', 'aren', 'i', 'am', 'been', 'no', 'your', 'with', 'itself', 'a', 'doing', 'ain', 'those', 'down', 'other', 'each', 'will', 'than', 'at', 'between', 'have', 'hasn', 'his', 'myself', 'during', 'be', 'by', \"should've\", 'before', 'needn', 'he', 's', 'has', 'wasn', 're', 'and', 'him', 'didn', 'below', \"aren't\", 'being', \"doesn't\", 'until', \"shouldn't\", 'weren', 'are', 'this', 'from', 'nor', \"shan't\", \"it's\", 'very', 'm', 'them', 'do', 'just', 'over', 'she', 'their', 'hadn', \"needn't\", 'whom', \"she's\", 'does', 'all', 'was', 'haven', \"weren't\", \"you're\", 'as', 'few', 'so', 'where', 'mightn', 'there', 'is', 'these', 'more', 'in', \"isn't\", \"haven't\", 'd', 'some', 'mustn', 'above', 'himself', 'further', \"that'll\", \"don't\", 'why', 'up', 'that', 've', 'we', 'only', \"wasn't\", 'our', 'too', 'which', 'any', 'doesn', 'were', 'under', 'the', 'ours', 't', 'of', 'on', \"wouldn't\", 'most', 'yourself', 'don', 'should', 'y', 'or', 'shouldn', 'out', 'having', 'an', 'against', 'off', 'herself', 'once', 'me', 'themselves', 'll', \"mustn't\", \"won't\", 'but', 'shan', 'not', 'own', 'into', \"you've\", 'here', 'won', 'both', 'who', 'to', 'now', 'ourselves', \"you'd\", 'they', \"hadn't\", 'did', 'had', \"hasn't\", 'when', \"mightn't\", 'what', 'about', 'ma', 'how', 'my', 'couldn', 'wouldn', 'while', 'isn', 'then', 'it', \"didn't\", 'you', 'yours', 'again', 'theirs', 'such', 'if', 'for', 'after', \"couldn't\", 'her', 'hers', 'its', 'can'}\n"
     ]
    }
   ],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gone', ',', 'runne', 'houses', ',', 'fall', 'vpon', 'knees', ',', 'pray', 'gods', 'intermit', 'plague', 'needs', 'must', 'light', 'ingratitude']\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "corpus2 = [[word for word in sent if not word in stops] for sent in corpus]\n",
    "print(corpus2[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#punctuations\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gone', 'runne', 'houses', 'fall', 'vpon', 'knees', 'pray', 'gods', 'intermit', 'plague', 'needs', 'must', 'light', 'ingratitude']\n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "corpus3 = [[word for word in sent if not word in string.punctuation] for sent in corpus2]\n",
    "print(corpus3[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3032 2917\n"
     ]
    }
   ],
   "source": [
    "#making vocabulary\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabulary = list(set(flatten(corpus3)))\n",
    "vocabulary.append('<unk>')\n",
    "print(len(set(flatten(corpus))), len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating ordering of words using word dictionary and index dictionary\n",
    "d_word = {'<unk>' : 0} \n",
    "\n",
    "for word in vocabulary:\n",
    "    d_word[word] = d_word.get(word, 0) + len(d_word)\n",
    "\n",
    "d_ind = {v:k for k,v in d_word.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('what', ',', 'know', 'you', 'not', '(', 'being')\n",
      "('<dummy>', '<dummy>', '<dummy>', '[', 'the', 'tragedie', 'of')\n"
     ]
    }
   ],
   "source": [
    "#defining window size\n",
    "win_size = 3 #3 surrounding words on both sides. \n",
    "act_win_size = 7 #Actual window size as defined above=7. \n",
    "#<dummy> variable if it is a start or end of sentence and there are not 3 surrounding words \n",
    "windows = flatten([list(nltk.ngrams(['<dummy>'] * win_size + c + ['<dummy>'] * win_size, act_win_size)) for c in corpus])\n",
    "print(windows[50])\n",
    "print(windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting training data i.e. target, context pairs\n",
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(act_win_size):\n",
    "        if i == win_size or window[i] == '<dummy>': \n",
    "            continue\n",
    "        train_data.append((window[win_size], window[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(word):\n",
    "    return Variable(LongTensor([d_word[word]]) if d_word.get(word) is not None else LongTensor([d_word[\"<unk>\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gpu_true = torch.cuda.is_available() #checking if GPU is available\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "#declaring tensors\n",
    "FloatTensor = torch.cuda.FloatTensor if gpu_true else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if gpu_true else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if gpu_true else torch.ByteTensor\n",
    "print(gpu_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing training data \n",
    "X_label = []\n",
    "Y_label = []\n",
    "from torch.autograd import Variable\n",
    "for t in train_data:\n",
    "    X_label.append(check_word(t[0]).view(1, -1))\n",
    "    Y_label.append(check_word(t[1]).view(1, -1))\n",
    "train_data = list(zip(X_label, Y_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "\n",
    "        self.embedding_v.weight.data.uniform_(-1, 1) # init\n",
    "        self.embedding_u.weight.data.uniform_(0, 0) # init\n",
    "        #self.out = nn.Linear(projection_dim,vocab_size)\n",
    "    def forward(self, center_words,target_words, outer_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        outer_embeds = self.embedding_u(outer_words) # B x V x D\n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1xD * BxDx1 => Bx1\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # BxVxD * BxDx1 => BxV\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_seq(seq):\n",
    "    indices = list(map(lambda w: d_word[w] if d_word.get(w) is not None else d_word[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "losses = []\n",
    "model = Skipgram(len(d_word), EMBEDDING_SIZE)\n",
    "if gpu_true:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 7.10\n",
      "Epoch : 10, mean_loss : 5.95\n",
      "Epoch : 20, mean_loss : 5.86\n",
      "Epoch : 30, mean_loss : 5.83\n",
      "Epoch : 40, mean_loss : 5.81\n",
      "Epoch : 50, mean_loss : 5.80\n",
      "Epoch : 60, mean_loss : 5.80\n",
      "Epoch : 70, mean_loss : 5.79\n",
      "Epoch : 80, mean_loss : 5.79\n",
      "Epoch : 90, mean_loss : 5.79\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs) \n",
    "        targets = torch.cat(targets) \n",
    "        vocabs = index_seq(list(vocabulary)).expand(inputs.size(0), len(vocabulary))  # B x V\n",
    "        model.zero_grad()\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    if gpu_true:\n",
    "        target_V = model.prediction(check_word(target))\n",
    "    else:\n",
    "        target_V = model.prediction(check_word(target))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: continue\n",
    "        \n",
    "        if gpu_true:\n",
    "            vector = model.prediction(check_word(list(vocab)[i]))\n",
    "        else:\n",
    "            vector = model.prediction(check_word(list(vocab)[i]))\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] \n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10] # sort by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'superstitious'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = random.choice(list(vocabulary))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['piercing', 0.9944832921028137],\n",
       " ['cursed', 0.9944651126861572],\n",
       " ['humours', 0.9907364845275879],\n",
       " ['storme', 0.9884722828865051],\n",
       " ['imminent', 0.9882675409317017],\n",
       " ['darts', 0.9843094348907471],\n",
       " ['vnto', 0.9819858074188232],\n",
       " ['lusty', 0.9801998138427734],\n",
       " ['creatures', 0.9793422222137451],\n",
       " ['tinctures', 0.9793134927749634]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "word_similarity(test, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. CBOW (Continuous Bag Of Words)\n",
    "This model is opposite of Skip-gram model. In this model, the context words are the input and we try to predict the target word. \n",
    "<img src=\"cbow.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" width = 400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of cboe is similar. There are various optimization methods for improving the performance of cbow such as negative sampling, hierarchial softmax, etc.<br>\n",
    "Gensim library has both the implementation of skip-gram as well as cbow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between farre and done is(cbow): 0.012073921747163617\n",
      "Cosine similarity between farre and done is(skipgram): 0.9863659272324679\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "cbow = gensim.models.Word2Vec(corpus3, min_count = 1, size = 100, window = 5) \n",
    "skipgram = gensim.models.Word2Vec(corpus3, min_count = 1, size = 100, window = 5, sg = 1)\n",
    "word1 = random.choice(list(vocabulary))\n",
    "word2 = random.choice(list(vocabulary))\n",
    "print(\"Cosine similarity between\", word1, \"and\", word2, \"is(cbow):\", cbow.wv.similarity(word1, word2)) \n",
    "print(\"Cosine similarity between\", word1, \"and\", word2, \"is(skipgram):\", skipgram.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it is computationally expensive to train a neural network and also the word vectors are limited to a small corpus. The general method is to load pre-calculated word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a variety of tasks with word vectors such as finding analogy, removing bias such as gender bias from vectors, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c1\n",
    "2. https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "3. https://www.kaggle.com/watts2/glove6b50dtxt\n",
    "4. https://github.com/DSKSD/DeepNLP-models-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
